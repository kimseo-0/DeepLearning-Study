{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852ea2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import transforms         # ì´ë¯¸ì§€ ë³€í˜•\n",
    "from torch.utils.data.dataloader import DataLoader  # train - test ë¶„ë¦¬\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f39cff",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e72d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(32),\n",
    "    transforms.Normalize((0.5),(1.0)) # í‰ê· , í‘œì¤€í¸ì°¨\n",
    "])\n",
    "\n",
    "train_data = MNIST(root='./', train=True, download=True, transform=data_transform) \n",
    "test_data = MNIST(root='./', train=False, download=True, transform=data_transform) \n",
    "# transform : ë°ì´í„° ì „ì²˜ë¦¬í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01d39e",
   "metadata": {},
   "source": [
    "## 3. ë°°ì¹˜ ì‚¬ì´ì¦ˆì— ë”°ë¥¸ ë°ì´í„° ë¶„ë¦¬`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61d6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True) \n",
    "test_loader = DataLoader(test_data, batch_size=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239cd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(train_loader))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196a2ba",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0136385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet, self).__init__()\n",
    "\n",
    "        # convolutions\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "\n",
    "        # fully connection\n",
    "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)         \n",
    "        x = F.tanh(x)             # 6 28 28\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # subsampling 6 14 14\n",
    "\n",
    "        x = self.conv2(x)         # 16 10 10 ((14 - 5 + (2 * 0)) / 1 + 1)\n",
    "        x = F.tanh(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # 16 5 5\n",
    "\n",
    "        x = self.conv3(x)         # 120 1 1 ((5 - 5 + (2 * 0)) / 1 + 1)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        x = x.view(-1, 120)         # 120 ì´ë¯¸ì§€ í‰íƒ„í™”\n",
    "\n",
    "        x = self.fc1(x)             # 84\n",
    "        x = F.tanh(x)               # í™œì„±í•¨ìˆ˜\n",
    "\n",
    "        x = self.fc2(x)             # 10\n",
    "        nn.Linear(in_features=84, out_features=10)\n",
    "        x = F.tanh(x)               # í™œì„±í•¨ìˆ˜\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Lenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet(nn.Module): # ğŸ“Œ ëª¨ë¸ëª…(nn.Module) , ê´„í˜¸ ì•ˆì— nn.Module ì„ ë°˜ë“œì‹œ ì ì„ ê²ƒ\n",
    "    def __init__(self): # ğŸ“Œ __init__(self) ë¥¼ ë°˜ë“œì‹œ í•´ì•¼í•¨\n",
    "        super(Lenet, self).__init__() # ğŸ“Œ super(ëª¨ë¸ëª…, self).__init__() ë°˜ë“œì‹œ í•´ì•¼í•¨\n",
    "\n",
    "        # convolutions\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "\n",
    "        # fully connection\n",
    "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10) # â­â­ out_features ëŠ” ë‚´ê°€ ë¶„ë¥˜í•˜ê³  ì‹¶ì€ ì •ë‹µ\n",
    "    \n",
    "    def forward(self, x):  # ğŸ“Œ forward ì•ˆì— self ë¥¼ ë°˜ë“œì‹œ ì ì–´ì•¼í•¨\n",
    "\n",
    "        x = self.conv1(x)    \n",
    "        x = F.tanh(x)   # í™œì„±í•¨ìˆ˜                                # 6 28 28\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # subsampling 6 14 14\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.tanh(x)   # í™œì„±í•¨ìˆ˜\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # 16 5 5\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        x = torch.reshape(x, (-1, 120)) # ì´ë¯¸ì§€ í‰íƒ„í™”\n",
    "\n",
    "        x = self.fc1(x)             # ì„ í˜•í•¨ìˆ˜ 84\n",
    "        x = F.tanh(x)               # í™œì„±í•¨ìˆ˜\n",
    "\n",
    "        x = self.fc2(x)             # ì„ í˜•í•¨ìˆ˜ 10\n",
    "        x = F.tanh(x)               # í™œì„±í•¨ìˆ˜\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208f7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257dc4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë‘ gpuì— ì˜¬ë¼ê°€ê²Œ í•˜ëŠ” ì½”ë“œ\n",
    "\n",
    "# if torch.backends.cuda.is_available():\n",
    "#   torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de39a36",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "step = 0\n",
    "for epoch in range(epochs):\n",
    "    for data, label in train_loader: # [(data, label)]\n",
    "        optim.zero_grad() # ğŸ“Œ ìµœì í™” í•¨ìˆ˜ë¥¼ ì´ˆê¸°í™” í•´ì•¼í•¨ (í•œ ë²ˆ í•™ìŠµì‹œ ë§ˆë‹¤)\n",
    "\n",
    "        # 1) ìˆœì „íŒŒ\n",
    "        pred = model(data.to(device)) # ë°ì´í„° ìœ„ì¹˜ ì²´í¬\n",
    "\n",
    "        # 2) ì†ì‹¤ ê³„ì‚°\n",
    "        loss = criterion(pred, label.to(device)) # ë°ì´í„° ìœ„ì¹˜ ì²´í¬\n",
    "\n",
    "        # 3) ì—­ì „íŒŒ\n",
    "        loss.backward()\n",
    "        optim.step() # 4) íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "\n",
    "        # tensorboardì— ë°ì´í„° ì¶”ê°€\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "        step += 1\n",
    "\n",
    "    print(f\"{epoch + 1} loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea6aa91",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì €ì¥í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bd18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "joblib.dump(model, 'models/number_image_cnn_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f940f",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "loaded_model = joblib.load('models/number_image_cnn_model.pkl')\n",
    "\n",
    "loaded_model.eval() # ëª¨ë¸ì„ ì¶”ë¡ ìš©ìœ¼ë¡œ ì „í™˜í•˜ê²Œ í•˜ëŠ” ì½”ë“œ\n",
    "\n",
    "falut_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_corr = 0\n",
    "    for images, labels in test_loader:\n",
    "        X = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = loaded_model(X)\n",
    "        _, pred = torch.max(preds.data, dim = 1)\n",
    "        \n",
    "        result = (pred == labels)\n",
    "        total_corr += (result).sum().item()\n",
    "\n",
    "        for i, re in enumerate(result):\n",
    "            if re == False:\n",
    "                falut_data.append({\n",
    "                    'image' : images[i],\n",
    "                    'pred' : pred[i],\n",
    "                    'label' :labels[i]\n",
    "                })\n",
    "\n",
    "print(f'ì •í™•ë„ : {total_corr / len(test_data.targets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb2674",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe08a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 1. ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
    "image_path = 'images/4.jpg'\n",
    "\n",
    "# 2. ì´ë¯¸ì§€ ë³€í™˜(transform) íŒŒì´í”„ë¼ì¸ ì •ì˜\n",
    "# PyTorch ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì •ì˜\n",
    "# ToTensor()ëŠ” PIL Imageë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ë©°, í”½ì…€ ê°’ì„ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),              # ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((32, 32)),        # ì´ë¯¸ì§€ë¥¼  ë¦¬ì‚¬ì´ì¦ˆ\n",
    "    transforms.Normalize((0.5),(1.0))   # í‰ê· , í‘œì¤€í¸ì°¨\n",
    "])\n",
    "\n",
    "with Image.open(image_path) as image:\n",
    "        print(f\"PIL Image í¬ê¸°: {image.size}\")\n",
    "        print(f\"PIL Image ëª¨ë“œ: {image.mode}\")\n",
    "\n",
    "        image = image.convert('L')\n",
    "        \n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        # 4. ì •ì˜í•œ ë³€í™˜(transform)ì„ ì´ë¯¸ì§€ì— ì ìš©\n",
    "        tensor_image = transform(image)\n",
    "\n",
    "# 5. í…ì„œ ì •ë³´ í™•ì¸\n",
    "print(\"\\nì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜ ì™„ë£Œ:\")\n",
    "print(f\"í…ì„œ í¬ê¸°(size): {tensor_image.size}\")\n",
    "print(f\"í…ì„œ ë°ì´í„° íƒ€ì…: {tensor_image.dtype}\")\n",
    "\n",
    "# 6. ëª¨ë¸ ì˜ˆì¸¡\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loaded_model.to(device)\n",
    "\n",
    "tensor_image = tensor_image.unsqueeze(dim=0)\n",
    "preds = loaded_model(tensor_image.to(device))\n",
    "_, pred = torch.max(preds.data, dim=1)\n",
    "print(f\"ì˜ˆì¸¡ ê²°ê³¼: {pred.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning-Study (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
